paper: Dissociating language and thought in large language models  a cognitive perspective


The author considers that language ability can be divided into "formal competence" and "functional competence". 
Formal competence is the ability of handing pure language itself, 
and functional competence includes formal reasoning, world knowledge, situation modeling, social cognition/communicative intention, etc.

作者提出，语言能力要分为“形式能力”和“功能能力”。形式能力是语言本身的能力，功能能力包含形式化推理，世界知识，情景建模，交流意图等。


Just from my personal viewpoint, from bottom to top, from objecvtive-world to symbol representation, should be:

L3:  Pure Language Competence   

L2:  Symbol Representation Cometence

L1:  Objective and Subjective World and Motion including Object,Attribute,Relationship,Motion

     World -> World Model -> Generalized Computation / Thinking / Preditciton / AI-shortcut   [for task-completion:  situation modeling  ->  for language-communication: ->  social cognition/communicative intention]



Abstraction:

Today's Large Language Models (LLMs) typically produce coherent, grammatical, and seemingly meaningful paragraphs of text.
This achievement has sparked speculation that these networks are — or will soon be — "thinking machines" capable of performing tasks that require abstract knowledge and reasoning.
Here, we review LLM's capabilities by considering how they perform in two different aspects of language use:
"formal language competence", which includes knowledge of the rules and patterns of a given language,
and "functional language skills," a range of cognitive abilities necessary for real-world language understanding and use.
Using evidence from cognitive neuroscience,
We show that human formal abilities rely on specialized language processing mechanisms,
Functional competencies, on the other hand, recruit the extralingual, linguistic abilities that make up the human mind, such as formal reasoning, world knowledge, situational modeling, and social cognition.
In line with this distinction, LLM exhibits impressive (albeit imperfect) performance on tasks that require formal language proficiency,
But it failed in many tests that required functional competence.
Based on this evidence, we believe that:
(1) contemporary LLM should be taken seriously as a model of formal language skills; 
(2) Mastering models used in real-life languages requires not only the incorporation or development of core language modules, but also the ability to merge or develop multiple non-language-specific cognitive abilities required for modeling thinking.
Overall, the distinction between formal language competence and functional language competence helps clarify the discourse surrounding the potential of LLM and provides a pathway for building models for understanding and using language in human-like ways.


当今的大型语言模型 (LLM) 通常会生成连贯、符合语法且看似有意义的文本段落。
这一成就引发了人们的猜测：这些网络是——或即将成为——“思考机器”，能够执行需要抽象知识和推理的任务。
在这里，我们通过考虑LLM在语言使用的两个不同方面的表现来回顾他们的能力：
“形式语言能力”，其中包括给定语言的规则和模式的知识，
以及“功能语言能力”，一系列认知能力现实世界中语言理解和使用所必需的。
利用认知神经科学的证据，
我们表明人类的形式能力依赖于专门的语言处理机制，
而功能能力则招募构成人类思维的多种语言外能力，例如形式推理、世界知识、情境建模和社会认知。
与这一区别相一致，LLM在需要正式语言能力的任务上表现出令人印象深刻（尽管不完美）的表现，
但在许多需要功能能力的测试中却失败了。
基于这一证据，我们认为：
（1）当代LLM应该被认真对待，作为正式语言技能的典范； 
（2）掌握现实生活语言使用的模型不仅需要合并或开发核心语言模块，还需要合并或开发建模思维所需的多种非特定语言的认知能力。
总体而言，形式语言能力和功能语言能力之间的区别有助于澄清围绕LLM潜力的论述，并为构建以类人方式理解和使用语言的模型提供了一条途径。







