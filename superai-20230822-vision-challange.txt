如果我们想构造：统一+理想+终极/立体视觉，有几个问题，想请教请教：

关于stereo vision task定义（逼近物理现实主要需解决的关键点），和stereo vision data利用？
我个人从这几个方面来理解现在的表示和渲染上的挑战：
1.1 逼近物理现实
1.1.1 含时/动态的对象和场景，比如k-planes在研究的
#问1：k-planes等主要还是在解决刚体含时动态，包括纹理的变化； 如果对象在动态方面更有挑战，”更有挑战“包含：非刚体的形变，水波的运动，头发丝风吹手抚的互作，瑞士钟表等严格机械运动，... 这种时候，重心都不在shape和texture了，而在于运动了，nerf这种方向比如k个planes都不够了，是不是要扩展到更泛泛的vision-task的定义和研究上，比如某种体现特定运动约束的连续函数来独立或联合表示运动？
1.1.2 多对象和复杂场景，它们有严格的相对位置关系
现在大多还是在2D图片的输入上分割了，把要构造的对象给mask出来；如果是复杂的场景，还要得到3D对象级别独立完整合理的对象，挑战也很大。
1.1.3 特殊物理特性的对象（除了透明，还有游戏引擎特殊处理的水波烟花，头发等）
#问2：特殊物理特性的对象，这一大方向，是不是nerf这种大方向的表示和渲染，就不太合适。 比如头发，mesh/obj就几乎难以表示，而Nvidia的那种头发效果就非常好（keywords: nvidia hair admm interactive)
1.2 应用中的需求或者限制
1.2.1 相机位置的估计
从NeRF这条路走过来的，可能由于NeRF的任务定位在多视觉合成，所以绝大部分的研究，都在假设已知相机位置，采用colmap或者blender里的合成对象来直接使用camera pose。
然而我们如果想更完整更自动的立体视觉系统的时候，或者参考人类一旦进入一个3D空间，对场景中每个对象其实立刻完成了自己的观察定位。
现在零星有一些camera pose esitmation的工作，无论是直接估计，还是联合优化nerf和camera pose，基本都是采用和原生nerf相同的数据表示方式（形式可能不一样，但本质一样，可转换。）
关于camera pose，我有几个理解：
A. 物理现实中：从纯物理角度看，肯定有精准的camera pose；从神经认知角度看，未必需要camera pose很精准。（如果认为大脑对视觉是单双目图像入，然后所有立体都是recall不是reconstuct，其实也不可能reconsuction。)
B. 具体的计算机视觉任务中，其实可以不要或者弱化camera pose信息，比如偏generation的任务，比如端到端输出mesh，比如就只需要其他一些视角的图像。
#问3: 假定我们就需要明确的camera pose信息（明确，不一定是准确），我们应该：如何表示，如何计算？
A. 关于表示：
统一到相对比较绝对的世界坐标系/相机坐标系（这对我们数据处理和泛化很有挑战），还是对象和观察者之间的相对关系？
对象是真实（比例）大小，还是将任何对象（蚂蚁和大象）都归一化到一个统一的心理大小（01）,然后来表示camera pose距离(0|1infine|enough)和角度（360*360）？
是要精确的camera pose，还是模糊的信息？
B. 关于计算：
我其实很困惑：1）人类在立体空间观察姿态定位和立体空间重构，神经过程是什么？看看nerf是一个先pose后reconstuct（无论colmap还是nn pose），看nerf-camera-w/o，基本还是参数化camera i/e参数然后joint优化； 我理解除了联合，还可以能交错迭代优化； 2）如果我们要一定程度拜托物理学/图形学的标准做法，是不是可以归一化/简化/直接化某一组能够映射到现在camera pose的表示上的其他表示，以图更好的计算。
1.2.2 单/多图条件生成/重构，先验利用：
3D重构本来是个重构问题，如果视角足够多、姿态足够准，本来就是一个表示和渲染下的强拟合问题，无论是隐式还是显式mesh；正因为只有提示文本，或者单/多视角图片，活生生被掰成了一个偏（条件）生成的问题。
#问4：先验哪里来？先验/约束如何有效利用？
A. 有来自于数据的：文本，图像，文本-图像，；还有立体模型（现在最大的应该是objaverse-xl + 上海open vision lab的一个。）。 其中objaverse现在主要通过zero-123和pixel-nerf间接的利用；还没有看到类似一个diffusion-nerf甚至dissusion-[nerf]-mesh的直接利用。（数据量大，模型大，算力很难搞了。）
B. 有来自知识的：主要表现在图形学方向loss形式的各种正则。
C. 网络上，主要diffusion，其次GAN，个别VAE的。
按照DL发展比较成功的经验，未来主要来自于data？ 现在objaverse这种间接模型的形式，还不是LLM中那种包揽了大半/或者全部逻辑的基础模型，特别downstream到mesh输出。
从我个人在单/少图构建”尽力真“的构造立体模型经验看（真：是和输入图像相似：就像某人/personal-id，不仅是该对象类别合理真实：像个真人类/category:human）。
比如：单张刘亦菲的人脸图，构造一个逼真的刘亦菲模型，在人类主观的感受”逼真中“，除了shape，其实更多的是texture。比如在smpl约束了人类人体/人脸信息，然后fit人的shape(比较准就行，刘亦菲可胖可瘦一点不太影响真实感，然后通过可微的（插值+拟合）方式，去fit皮肤，可以让loss足够小，刘亦菲足够以假乱真。
回看现在的主要的nerf方向论文的做法，不是这样；也不可能把更丰富的id-level的约束信息带入过去；简单的作为gen的条件利用一个抽象的laten不够，必须要最后对照input-image做强拟合才能充分利用input的信息。
1.2.3 其他各种工程性等需求
A. 多对象/场景复杂/场景大/无边界 （略）
场景大，不如自动驾驶场景做到街区和城市级别。
#问5: 当对象足够多，甚至超越objaverse，是超大的网络，还是在较大网络+verctor-db的模式？
B. 训练/推理足够快 （略）
C. 表示内存小 （略）
D. 渲染的质量好/各种质量不好的问题太多，解决思路也很多（硬算法的，粗细优化的） （略）
E. 对象可以操纵，对象之间有交互 （略）
F. 全流水线自动化 （略）
G. 输出为Mesh
其中输出为mesh利于在游戏引擎等场景中实时渲染使用，这里路线也很多：直接mesh-fit的，先NeRF然后各种提取算法的，走NeuS/VolSDF/...，...
#问6：如果结合one-shot-image，尽力端到端可微，Diffusion+NeRF的方式，以前很多是额外独立做mesh抽取，沿本文实现为One-Stage的Diffusion+[NeRF]+Mesh(shape+texture)，是不是应该更好？

#问0：更unified, 更逼近物理现实比如复杂运动/或动物视觉生理心理神经过程的stereo vision的算法全景是什么呢？


